{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "\n",
    "import yaml # for parsing hyperparameters\n",
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda')\n",
    "\n",
    "with open('hyperparameters.yaml') as f:\n",
    "    hp = yaml.safe_load(f)['hyperparameters']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, actions_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.pred = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size)\n",
    "            , nn.ReLU()\n",
    "            , nn.Linear(hidden_size, hidden_size)\n",
    "            , nn.ReLU()\n",
    "            , nn.Linear(hidden_size, actions_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pred(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env._max_episode_steps = 4000\n",
    "\n",
    "obs_size = env.observation_space.shape[0]\n",
    "actions_size = env.action_space.n\n",
    "\n",
    "net = Net(obs_size, hp['hidden_size'], actions_size)\n",
    "net = net.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "writer = SummaryWriter(comment='CartPole')\n",
    "\n",
    "ep = namedtuple('ep', field_names=['reward', 'steps'])\n",
    "ep_step = namedtuple('ep_step', field_names=['observation', 'action'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def generate_batch(env, net, batch_size):\n",
    "    batch = []\n",
    "    ep_reward = 0.\n",
    "    ep_steps = []\n",
    "    obs = env.reset()\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    while True:\n",
    "        if len(batch) == batch_size - 1 or len(batch) == batch_size/2 - 1:\n",
    "           env.render()\n",
    "\n",
    "        obs_t = torch.FloatTensor([obs])\n",
    "        obs_t = obs_t.cuda()\n",
    "        actions_t = softmax(net(obs_t))\n",
    "        actions = actions_t.cpu().data.numpy()[0]\n",
    "        action = np.random.choice(len(actions), p=actions)\n",
    "        next_obs, reward, is_done, _ = env.step(action)\n",
    "        ep_reward += reward\n",
    "        step = ep_step(obs, action)\n",
    "        ep_steps.append(step)\n",
    "\n",
    "        if is_done:\n",
    "            e = ep(ep_reward, ep_steps)\n",
    "            batch.append(e)\n",
    "            ep_reward = 0.\n",
    "            ep_steps = []\n",
    "            next_obs = env.reset()\n",
    "\n",
    "            if len(batch) == batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "\n",
    "        obs = next_obs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def filter_batch(batch, percentile):\n",
    "    rewards = list(map(lambda s: s.reward, batch)) # gets reward values\n",
    "    rewards_bound = np.percentile(rewards, percentile)\n",
    "    rewards_mean = float(np.mean(rewards))\n",
    "    train_obs = []\n",
    "    train_actions = []\n",
    "\n",
    "    for reward, steps in batch:\n",
    "        if reward < rewards_bound:\n",
    "            continue\n",
    "        train_obs.extend(map(lambda s: s.observation, steps)) # gets obs values\n",
    "        train_actions.extend(map(lambda s: s.action, steps)) # gets action values\n",
    "\n",
    "    train_obs_t = torch.FloatTensor(train_obs)\n",
    "    train_actions_t = torch.LongTensor(train_actions)\n",
    "\n",
    "    return train_obs_t, train_actions_t, rewards_bound, rewards_mean"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def train():\n",
    "    for iter_no, batch in enumerate(generate_batch(env, net, hp['batch_size'])):\n",
    "        obs_t, actions_t, reward_bound, reward_mean = filter_batch(batch, hp['keep_%'])\n",
    "\n",
    "        obs_t = obs_t.cuda()\n",
    "        actions_t = actions_t.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        action_score_t = net(obs_t)\n",
    "        loss_t = criterion(action_score_t, actions_t)\n",
    "        loss_t.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f'Epoch: {iter_no:} \\t Reward: {reward_mean:.2f} \\t Boundary: {reward_bound:.2f}')\n",
    "        writer.add_scalar('Loss', loss_t.item(), iter_no)\n",
    "        writer.add_scalar('Reward', reward_mean, iter_no)\n",
    "        writer.add_scalar('Boundary', reward_bound, iter_no)\n",
    "\n",
    "        if reward_mean > 4000:\n",
    "            print('Model reached reward limit..')\n",
    "            break\n",
    "\n",
    "    writer.close()\n",
    "    env.close()\n",
    "    torch.save(net.state_dict(), './model/model.weights')\n",
    "    print('Model Saved.')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t Reward: 19.78 \t Boundary: 34.20\n",
      "Epoch: 1 \t Reward: 22.38 \t Boundary: 36.40\n",
      "Epoch: 2 \t Reward: 21.28 \t Boundary: 31.00\n",
      "Epoch: 3 \t Reward: 21.59 \t Boundary: 40.80\n",
      "Epoch: 4 \t Reward: 24.47 \t Boundary: 39.70\n",
      "Epoch: 5 \t Reward: 29.94 \t Boundary: 50.60\n",
      "Epoch: 6 \t Reward: 27.84 \t Boundary: 44.00\n",
      "Epoch: 7 \t Reward: 27.09 \t Boundary: 49.60\n",
      "Epoch: 8 \t Reward: 32.47 \t Boundary: 61.30\n",
      "Epoch: 9 \t Reward: 29.88 \t Boundary: 51.40\n",
      "Epoch: 10 \t Reward: 33.28 \t Boundary: 55.80\n",
      "Epoch: 11 \t Reward: 31.72 \t Boundary: 45.00\n",
      "Epoch: 12 \t Reward: 34.00 \t Boundary: 63.00\n",
      "Epoch: 13 \t Reward: 32.66 \t Boundary: 46.50\n",
      "Epoch: 14 \t Reward: 36.72 \t Boundary: 63.90\n",
      "Epoch: 15 \t Reward: 35.12 \t Boundary: 59.40\n",
      "Epoch: 16 \t Reward: 37.53 \t Boundary: 58.60\n",
      "Epoch: 17 \t Reward: 36.28 \t Boundary: 52.60\n",
      "Epoch: 18 \t Reward: 37.66 \t Boundary: 77.30\n",
      "Epoch: 19 \t Reward: 40.94 \t Boundary: 64.00\n",
      "Epoch: 20 \t Reward: 35.91 \t Boundary: 60.70\n",
      "Epoch: 21 \t Reward: 42.91 \t Boundary: 64.90\n",
      "Epoch: 22 \t Reward: 44.78 \t Boundary: 82.70\n",
      "Epoch: 23 \t Reward: 44.66 \t Boundary: 72.20\n",
      "Epoch: 24 \t Reward: 44.34 \t Boundary: 66.40\n",
      "Epoch: 25 \t Reward: 44.25 \t Boundary: 71.30\n",
      "Epoch: 26 \t Reward: 44.31 \t Boundary: 64.80\n",
      "Epoch: 27 \t Reward: 52.12 \t Boundary: 76.20\n",
      "Epoch: 28 \t Reward: 45.72 \t Boundary: 70.60\n",
      "Epoch: 29 \t Reward: 51.25 \t Boundary: 100.40\n",
      "Epoch: 30 \t Reward: 55.22 \t Boundary: 94.30\n",
      "Epoch: 31 \t Reward: 70.16 \t Boundary: 121.80\n",
      "Epoch: 32 \t Reward: 64.91 \t Boundary: 99.70\n",
      "Epoch: 33 \t Reward: 59.06 \t Boundary: 87.70\n",
      "Epoch: 34 \t Reward: 52.00 \t Boundary: 83.90\n",
      "Epoch: 35 \t Reward: 70.00 \t Boundary: 98.80\n",
      "Epoch: 36 \t Reward: 69.06 \t Boundary: 104.60\n",
      "Epoch: 37 \t Reward: 73.66 \t Boundary: 125.90\n",
      "Epoch: 38 \t Reward: 66.59 \t Boundary: 108.90\n",
      "Epoch: 39 \t Reward: 74.56 \t Boundary: 133.50\n",
      "Epoch: 40 \t Reward: 76.41 \t Boundary: 129.80\n",
      "Epoch: 41 \t Reward: 84.53 \t Boundary: 128.30\n",
      "Epoch: 42 \t Reward: 85.50 \t Boundary: 128.00\n",
      "Epoch: 43 \t Reward: 91.69 \t Boundary: 137.50\n",
      "Epoch: 44 \t Reward: 114.28 \t Boundary: 188.40\n",
      "Epoch: 45 \t Reward: 91.28 \t Boundary: 155.10\n",
      "Epoch: 46 \t Reward: 110.50 \t Boundary: 202.20\n",
      "Epoch: 47 \t Reward: 105.31 \t Boundary: 179.60\n",
      "Epoch: 48 \t Reward: 123.78 \t Boundary: 206.00\n",
      "Epoch: 49 \t Reward: 127.41 \t Boundary: 223.00\n",
      "Epoch: 50 \t Reward: 132.38 \t Boundary: 197.70\n",
      "Epoch: 51 \t Reward: 140.12 \t Boundary: 215.70\n",
      "Epoch: 52 \t Reward: 127.28 \t Boundary: 174.90\n",
      "Epoch: 53 \t Reward: 154.31 \t Boundary: 266.20\n",
      "Epoch: 54 \t Reward: 134.50 \t Boundary: 219.80\n",
      "Epoch: 55 \t Reward: 170.25 \t Boundary: 309.60\n",
      "Epoch: 56 \t Reward: 129.44 \t Boundary: 215.90\n",
      "Epoch: 57 \t Reward: 160.12 \t Boundary: 282.20\n",
      "Epoch: 58 \t Reward: 143.91 \t Boundary: 259.70\n",
      "Epoch: 59 \t Reward: 144.75 \t Boundary: 229.30\n",
      "Epoch: 60 \t Reward: 181.41 \t Boundary: 277.30\n",
      "Epoch: 61 \t Reward: 175.00 \t Boundary: 269.90\n",
      "Epoch: 62 \t Reward: 149.16 \t Boundary: 224.50\n",
      "Epoch: 63 \t Reward: 163.78 \t Boundary: 255.50\n",
      "Epoch: 64 \t Reward: 182.31 \t Boundary: 268.90\n",
      "Epoch: 65 \t Reward: 179.38 \t Boundary: 248.80\n",
      "Epoch: 66 \t Reward: 229.16 \t Boundary: 391.30\n",
      "Epoch: 67 \t Reward: 195.09 \t Boundary: 360.70\n",
      "Epoch: 68 \t Reward: 206.12 \t Boundary: 347.50\n",
      "Epoch: 69 \t Reward: 219.19 \t Boundary: 340.40\n",
      "Epoch: 70 \t Reward: 220.19 \t Boundary: 356.20\n",
      "Epoch: 71 \t Reward: 199.12 \t Boundary: 313.50\n",
      "Epoch: 72 \t Reward: 206.38 \t Boundary: 297.70\n",
      "Epoch: 73 \t Reward: 199.75 \t Boundary: 267.30\n",
      "Epoch: 74 \t Reward: 242.03 \t Boundary: 344.90\n",
      "Epoch: 75 \t Reward: 254.31 \t Boundary: 396.50\n",
      "Epoch: 76 \t Reward: 291.16 \t Boundary: 468.80\n",
      "Epoch: 77 \t Reward: 305.00 \t Boundary: 414.30\n",
      "Epoch: 78 \t Reward: 334.44 \t Boundary: 467.80\n",
      "Epoch: 79 \t Reward: 324.50 \t Boundary: 537.50\n",
      "Epoch: 80 \t Reward: 296.16 \t Boundary: 479.40\n",
      "Epoch: 81 \t Reward: 305.25 \t Boundary: 386.80\n",
      "Epoch: 82 \t Reward: 305.53 \t Boundary: 511.20\n",
      "Epoch: 83 \t Reward: 341.06 \t Boundary: 474.90\n",
      "Epoch: 84 \t Reward: 362.47 \t Boundary: 571.50\n",
      "Epoch: 85 \t Reward: 343.25 \t Boundary: 508.50\n",
      "Epoch: 86 \t Reward: 335.06 \t Boundary: 526.50\n",
      "Epoch: 87 \t Reward: 371.34 \t Boundary: 608.90\n",
      "Epoch: 88 \t Reward: 367.16 \t Boundary: 639.50\n",
      "Epoch: 89 \t Reward: 422.72 \t Boundary: 805.30\n",
      "Epoch: 90 \t Reward: 395.25 \t Boundary: 692.40\n",
      "Epoch: 91 \t Reward: 415.50 \t Boundary: 667.30\n",
      "Epoch: 92 \t Reward: 456.28 \t Boundary: 660.50\n",
      "Epoch: 93 \t Reward: 360.00 \t Boundary: 593.20\n",
      "Epoch: 94 \t Reward: 401.78 \t Boundary: 730.60\n",
      "Epoch: 95 \t Reward: 492.50 \t Boundary: 788.40\n",
      "Epoch: 96 \t Reward: 480.56 \t Boundary: 874.70\n",
      "Epoch: 97 \t Reward: 479.06 \t Boundary: 742.90\n",
      "Epoch: 98 \t Reward: 396.62 \t Boundary: 613.80\n",
      "Epoch: 99 \t Reward: 526.25 \t Boundary: 805.50\n",
      "Epoch: 100 \t Reward: 467.91 \t Boundary: 755.80\n",
      "Epoch: 101 \t Reward: 493.06 \t Boundary: 787.40\n",
      "Epoch: 102 \t Reward: 524.34 \t Boundary: 924.20\n",
      "Epoch: 103 \t Reward: 510.41 \t Boundary: 821.90\n",
      "Epoch: 104 \t Reward: 573.91 \t Boundary: 923.80\n",
      "Epoch: 105 \t Reward: 617.12 \t Boundary: 896.00\n",
      "Epoch: 106 \t Reward: 529.22 \t Boundary: 914.30\n",
      "Epoch: 107 \t Reward: 527.75 \t Boundary: 867.30\n",
      "Epoch: 108 \t Reward: 482.44 \t Boundary: 862.40\n",
      "Epoch: 109 \t Reward: 677.38 \t Boundary: 1138.10\n",
      "Epoch: 110 \t Reward: 673.81 \t Boundary: 1072.80\n",
      "Epoch: 111 \t Reward: 704.38 \t Boundary: 1212.50\n",
      "Epoch: 112 \t Reward: 621.38 \t Boundary: 986.40\n",
      "Epoch: 113 \t Reward: 749.84 \t Boundary: 1246.60\n",
      "Epoch: 114 \t Reward: 718.56 \t Boundary: 992.50\n",
      "Epoch: 115 \t Reward: 599.06 \t Boundary: 1004.00\n",
      "Epoch: 116 \t Reward: 759.25 \t Boundary: 1331.90\n",
      "Epoch: 117 \t Reward: 852.62 \t Boundary: 1464.70\n",
      "Epoch: 118 \t Reward: 770.47 \t Boundary: 1359.20\n",
      "Epoch: 119 \t Reward: 899.00 \t Boundary: 1730.30\n",
      "Epoch: 120 \t Reward: 904.69 \t Boundary: 1475.30\n",
      "Epoch: 121 \t Reward: 852.91 \t Boundary: 1471.20\n",
      "Epoch: 122 \t Reward: 855.75 \t Boundary: 1660.70\n",
      "Epoch: 123 \t Reward: 928.41 \t Boundary: 1438.50\n",
      "Epoch: 124 \t Reward: 942.09 \t Boundary: 1943.90\n",
      "Epoch: 125 \t Reward: 935.88 \t Boundary: 1905.90\n",
      "Epoch: 126 \t Reward: 821.31 \t Boundary: 1346.30\n",
      "Epoch: 127 \t Reward: 810.47 \t Boundary: 1577.70\n",
      "Epoch: 128 \t Reward: 1188.56 \t Boundary: 2769.00\n",
      "Epoch: 129 \t Reward: 1068.97 \t Boundary: 1940.90\n",
      "Epoch: 130 \t Reward: 884.53 \t Boundary: 1394.10\n",
      "Epoch: 131 \t Reward: 808.41 \t Boundary: 1074.40\n",
      "Epoch: 132 \t Reward: 728.72 \t Boundary: 1059.30\n",
      "Epoch: 133 \t Reward: 1037.91 \t Boundary: 1665.50\n",
      "Epoch: 134 \t Reward: 1279.59 \t Boundary: 2449.20\n",
      "Epoch: 135 \t Reward: 1127.97 \t Boundary: 2226.70\n",
      "Epoch: 136 \t Reward: 1010.12 \t Boundary: 1544.50\n",
      "Epoch: 137 \t Reward: 857.62 \t Boundary: 1379.30\n",
      "Epoch: 138 \t Reward: 664.06 \t Boundary: 1050.10\n",
      "Epoch: 139 \t Reward: 886.38 \t Boundary: 1399.00\n",
      "Epoch: 140 \t Reward: 725.75 \t Boundary: 1158.80\n",
      "Epoch: 141 \t Reward: 830.94 \t Boundary: 1325.30\n",
      "Epoch: 142 \t Reward: 778.91 \t Boundary: 1206.90\n",
      "Epoch: 143 \t Reward: 792.59 \t Boundary: 1525.40\n",
      "Epoch: 144 \t Reward: 742.94 \t Boundary: 1223.30\n",
      "Epoch: 145 \t Reward: 705.53 \t Boundary: 1175.20\n",
      "Epoch: 146 \t Reward: 881.62 \t Boundary: 1367.30\n",
      "Epoch: 147 \t Reward: 879.53 \t Boundary: 1606.70\n",
      "Epoch: 148 \t Reward: 861.03 \t Boundary: 1559.00\n",
      "Epoch: 149 \t Reward: 847.75 \t Boundary: 1780.00\n",
      "Epoch: 150 \t Reward: 1130.34 \t Boundary: 2667.10\n",
      "Epoch: 151 \t Reward: 972.28 \t Boundary: 1649.70\n",
      "Epoch: 152 \t Reward: 991.34 \t Boundary: 1821.70\n",
      "Epoch: 153 \t Reward: 1598.56 \t Boundary: 3077.80\n",
      "Epoch: 154 \t Reward: 1302.03 \t Boundary: 2878.50\n",
      "Epoch: 155 \t Reward: 1665.03 \t Boundary: 3921.80\n",
      "Epoch: 156 \t Reward: 1765.50 \t Boundary: 3701.20\n",
      "Epoch: 157 \t Reward: 1611.69 \t Boundary: 3102.20\n",
      "Epoch: 158 \t Reward: 1654.78 \t Boundary: 3022.70\n",
      "Epoch: 159 \t Reward: 1906.81 \t Boundary: 3420.10\n",
      "Epoch: 160 \t Reward: 1540.41 \t Boundary: 3052.30\n",
      "Epoch: 161 \t Reward: 1478.09 \t Boundary: 2922.60\n",
      "Epoch: 162 \t Reward: 1939.50 \t Boundary: 3736.00\n",
      "Epoch: 163 \t Reward: 2071.47 \t Boundary: 4000.00\n",
      "Epoch: 164 \t Reward: 1899.44 \t Boundary: 3415.10\n",
      "Epoch: 165 \t Reward: 2083.38 \t Boundary: 4000.00\n",
      "Epoch: 166 \t Reward: 2318.09 \t Boundary: 4000.00\n",
      "Epoch: 167 \t Reward: 2060.88 \t Boundary: 4000.00\n",
      "Epoch: 168 \t Reward: 2506.69 \t Boundary: 4000.00\n",
      "Epoch: 169 \t Reward: 2736.06 \t Boundary: 4000.00\n",
      "Epoch: 170 \t Reward: 2407.81 \t Boundary: 4000.00\n",
      "Epoch: 171 \t Reward: 2911.06 \t Boundary: 4000.00\n",
      "Epoch: 172 \t Reward: 2773.56 \t Boundary: 4000.00\n",
      "Epoch: 173 \t Reward: 2713.06 \t Boundary: 4000.00\n",
      "Epoch: 174 \t Reward: 1909.88 \t Boundary: 3815.60\n",
      "Epoch: 175 \t Reward: 1814.41 \t Boundary: 3947.70\n",
      "Epoch: 176 \t Reward: 2277.78 \t Boundary: 4000.00\n",
      "Epoch: 177 \t Reward: 2660.81 \t Boundary: 4000.00\n",
      "Epoch: 178 \t Reward: 2547.66 \t Boundary: 4000.00\n",
      "Epoch: 179 \t Reward: 2254.28 \t Boundary: 4000.00\n",
      "Epoch: 180 \t Reward: 2671.91 \t Boundary: 4000.00\n",
      "Epoch: 181 \t Reward: 2874.91 \t Boundary: 4000.00\n",
      "Epoch: 182 \t Reward: 2439.97 \t Boundary: 4000.00\n",
      "Epoch: 183 \t Reward: 2805.31 \t Boundary: 4000.00\n",
      "Epoch: 184 \t Reward: 3265.62 \t Boundary: 4000.00\n",
      "Epoch: 185 \t Reward: 2666.53 \t Boundary: 4000.00\n",
      "Epoch: 186 \t Reward: 2824.75 \t Boundary: 4000.00\n",
      "Epoch: 187 \t Reward: 3299.53 \t Boundary: 4000.00\n",
      "Epoch: 188 \t Reward: 2362.31 \t Boundary: 4000.00\n",
      "Epoch: 189 \t Reward: 2973.09 \t Boundary: 4000.00\n",
      "Epoch: 190 \t Reward: 3096.56 \t Boundary: 4000.00\n",
      "Epoch: 191 \t Reward: 3238.12 \t Boundary: 4000.00\n",
      "Epoch: 192 \t Reward: 2704.16 \t Boundary: 4000.00\n",
      "Epoch: 193 \t Reward: 3045.69 \t Boundary: 4000.00\n",
      "Epoch: 194 \t Reward: 2887.31 \t Boundary: 4000.00\n",
      "Epoch: 195 \t Reward: 3242.91 \t Boundary: 4000.00\n",
      "Epoch: 196 \t Reward: 3387.34 \t Boundary: 4000.00\n",
      "Epoch: 197 \t Reward: 3031.50 \t Boundary: 4000.00\n",
      "Epoch: 198 \t Reward: 3333.94 \t Boundary: 4000.00\n",
      "Epoch: 199 \t Reward: 3417.88 \t Boundary: 4000.00\n",
      "Epoch: 200 \t Reward: 3130.62 \t Boundary: 4000.00\n",
      "Epoch: 201 \t Reward: 3202.03 \t Boundary: 4000.00\n",
      "Epoch: 202 \t Reward: 3149.88 \t Boundary: 4000.00\n",
      "Epoch: 203 \t Reward: 3178.22 \t Boundary: 4000.00\n",
      "Epoch: 204 \t Reward: 3363.00 \t Boundary: 4000.00\n",
      "Epoch: 205 \t Reward: 3191.22 \t Boundary: 4000.00\n",
      "Epoch: 206 \t Reward: 3423.06 \t Boundary: 4000.00\n",
      "Epoch: 207 \t Reward: 3229.41 \t Boundary: 4000.00\n",
      "Epoch: 208 \t Reward: 3046.62 \t Boundary: 4000.00\n",
      "Epoch: 209 \t Reward: 3099.97 \t Boundary: 4000.00\n",
      "Epoch: 210 \t Reward: 3160.66 \t Boundary: 4000.00\n",
      "Epoch: 211 \t Reward: 3513.62 \t Boundary: 4000.00\n",
      "Epoch: 212 \t Reward: 3571.75 \t Boundary: 4000.00\n",
      "Epoch: 213 \t Reward: 3522.88 \t Boundary: 4000.00\n",
      "Epoch: 214 \t Reward: 3602.66 \t Boundary: 4000.00\n"
     ]
    }
   ],
   "source": [
    "train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "deeplearning",
   "language": "python",
   "display_name": "DeepLearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}