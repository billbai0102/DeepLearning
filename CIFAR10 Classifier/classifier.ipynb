{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import autograd\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cpu\"\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda:0\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor()\n",
    "        , transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data'\n",
    "                                             , train=True\n",
    "                                             , download=True\n",
    "                                             , transform=transform)\n",
    "train_loader = DataLoader(train_dataset\n",
    "                          , batch_size=64\n",
    "                          , shuffle=True\n",
    "                          , num_workers=2)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data'\n",
    "                                            , train=False\n",
    "                                            , download=True\n",
    "                                            , transform=transform)\n",
    "test_loader = DataLoader(test_dataset\n",
    "                         , batch_size=64\n",
    "                         , shuffle=False\n",
    "                         , num_workers=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
     ]
    }
   ],
   "source": [
    "classes = train_dataset.classes\n",
    "print(classes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'builtin_function_or_method' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-28-0148bb2f5cd9>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[0mimages\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlabels\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdataiter\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnext\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 11\u001B[1;33m \u001B[0mimshow\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtorchvision\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mutils\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmake_grid\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mimages\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     12\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlabels\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-28-0148bb2f5cd9>\u001B[0m in \u001B[0;36mimshow\u001B[1;34m(img)\u001B[0m\n\u001B[0;32m      2\u001B[0m     \u001B[0mimage\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mimages\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcpu\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mclone\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdetach\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnumpy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m     \u001B[0mimg\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mimg\u001B[0m \u001B[1;33m/\u001B[0m \u001B[1;36m2\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;36m0.5\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 4\u001B[1;33m     \u001B[0mimg\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mimg\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcpu\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnumpy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      5\u001B[0m     \u001B[0mplt\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mimshow\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtranspose\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mimg\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m2\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m     \u001B[0mplt\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'builtin_function_or_method' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "def imshow(img: torch.Tensor):\n",
    "    image = images.cpu().clone().detach().numpy()\n",
    "    img = img / 2 + 0.5\n",
    "    img = img.cpu.numpy()\n",
    "    plt.imshow(np.transpose(img, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    print(classes[labels[i]], end=\"\\t\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=2, stride=1, padding=1)\n",
    "            , nn.BatchNorm2d(32)\n",
    "            , nn.ReLU()\n",
    "            , nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1)\n",
    "            , nn.BatchNorm2d(32)\n",
    "            , nn.ReLU()\n",
    "            , nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            , nn.Conv2d(32, 64, kernel_size=4, stride=1, padding=1)\n",
    "            , nn.BatchNorm2d(64)\n",
    "            , nn.ReLU()\n",
    "            , nn.Conv2d(64, 64, kernel_size=5, stride=1, padding=1)\n",
    "            , nn.BatchNorm2d(64)\n",
    "            , nn.ReLU()\n",
    "            , nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            # nn.Dropout(0.5)\n",
    "             nn.Linear(2304, 64)\n",
    "            , nn.BatchNorm1d(64)\n",
    "            , nn.ReLU()\n",
    "            , nn.Dropout(0.5)\n",
    "            , nn.Linear(64, 256)\n",
    "            , nn.BatchNorm1d(256)\n",
    "            , nn.ReLU()\n",
    "            , nn.Linear(256, 10) # 10 classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "Net(\n  (features): Sequential(\n    (0): Conv2d(3, 32, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (5): ReLU()\n    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (7): Conv2d(32, 64, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n    (8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (9): ReLU()\n    (10): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n    (11): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (12): ReLU()\n    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (classifier): Sequential(\n    (0): Linear(in_features=2304, out_features=64, bias=True)\n    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n    (3): Dropout(p=0.5, inplace=False)\n    (4): Linear(in_features=64, out_features=256, bias=True)\n    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (6): ReLU()\n    (7): Linear(in_features=256, out_features=10, bias=True)\n  )\n)"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net()\n",
    "net.cuda()\n",
    "net"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(params=net.parameters(), lr=0.001)\n",
    "steplr = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    net.train()\n",
    "    steplr.step()\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "\n",
    "        data = data.cuda()\n",
    "        target = target.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = net(data)\n",
    "        loss = criterion(out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (batch_idx + 1)% 100 == 0:\n",
    "            print(f'Train Epoch: {epoch + 1} --> {(batch_idx + 1) * len(data)}/{len(train_loader.dataset)} '\n",
    "                  f'({100.0 * (batch_idx + 1) / len(train_loader):.0f}%)\\tLoss: {loss.data.item():.6f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def evaluate(data_loader: DataLoader):\n",
    "    net.eval()\n",
    "    LOSS = 0\n",
    "    CORRECT = 0\n",
    "\n",
    "    for data, target in data_loader:\n",
    "        data, target = Variable(data), Variable(target)\n",
    "\n",
    "        data = data.cuda()\n",
    "        target = target.cuda()\n",
    "\n",
    "        out = net(data)\n",
    "\n",
    "        LOSS += F.cross_entropy(out, target, size_average=False).data.item()\n",
    "        pred = out.data.max(1, keepdim=True)[1]\n",
    "        CORRECT += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    LOSS /= len(data_loader.dataset)\n",
    "\n",
    "    print(f'\\nLoss: {LOSS:.4f} \\t Val Accuracy: {CORRECT}/{len(data_loader.dataset)} '\n",
    "          f'({100.0 * CORRECT / len(data_loader.dataset):.3f}%)\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 --> 6400/50000 (13%)\tLoss: 1.690735\n",
      "Train Epoch: 1 --> 12800/50000 (26%)\tLoss: 1.585959\n",
      "Train Epoch: 1 --> 19200/50000 (38%)\tLoss: 1.251005\n",
      "Train Epoch: 1 --> 25600/50000 (51%)\tLoss: 1.055244\n",
      "Train Epoch: 1 --> 32000/50000 (64%)\tLoss: 1.088808\n",
      "Train Epoch: 1 --> 38400/50000 (77%)\tLoss: 1.029568\n",
      "Train Epoch: 1 --> 44800/50000 (90%)\tLoss: 1.050050\n",
      "\n",
      "Loss: 0.8918 \t Val Accuracy: 6879/10000 (68.790%)\n",
      "\n",
      "Train Epoch: 2 --> 6400/50000 (13%)\tLoss: 0.925296\n",
      "Train Epoch: 2 --> 12800/50000 (26%)\tLoss: 0.979385\n",
      "Train Epoch: 2 --> 19200/50000 (38%)\tLoss: 0.998021\n",
      "Train Epoch: 2 --> 25600/50000 (51%)\tLoss: 1.077074\n",
      "Train Epoch: 2 --> 32000/50000 (64%)\tLoss: 1.031958\n",
      "Train Epoch: 2 --> 38400/50000 (77%)\tLoss: 0.834502\n",
      "Train Epoch: 2 --> 44800/50000 (90%)\tLoss: 0.852833\n",
      "\n",
      "Loss: 0.7825 \t Val Accuracy: 7293/10000 (72.930%)\n",
      "\n",
      "Train Epoch: 3 --> 6400/50000 (13%)\tLoss: 1.319555\n",
      "Train Epoch: 3 --> 12800/50000 (26%)\tLoss: 1.025064\n",
      "Train Epoch: 3 --> 19200/50000 (38%)\tLoss: 0.742737\n",
      "Train Epoch: 3 --> 25600/50000 (51%)\tLoss: 0.856320\n",
      "Train Epoch: 3 --> 32000/50000 (64%)\tLoss: 0.852655\n",
      "Train Epoch: 3 --> 38400/50000 (77%)\tLoss: 0.639643\n",
      "Train Epoch: 3 --> 44800/50000 (90%)\tLoss: 0.775406\n",
      "\n",
      "Loss: 0.7320 \t Val Accuracy: 7453/10000 (74.530%)\n",
      "\n",
      "Train Epoch: 4 --> 6400/50000 (13%)\tLoss: 0.841066\n",
      "Train Epoch: 4 --> 12800/50000 (26%)\tLoss: 0.650230\n",
      "Train Epoch: 4 --> 19200/50000 (38%)\tLoss: 0.753017\n",
      "Train Epoch: 4 --> 25600/50000 (51%)\tLoss: 0.551642\n",
      "Train Epoch: 4 --> 32000/50000 (64%)\tLoss: 0.607922\n",
      "Train Epoch: 4 --> 38400/50000 (77%)\tLoss: 0.738087\n",
      "Train Epoch: 4 --> 44800/50000 (90%)\tLoss: 0.639199\n",
      "\n",
      "Loss: 0.7611 \t Val Accuracy: 7412/10000 (74.120%)\n",
      "\n",
      "Train Epoch: 5 --> 6400/50000 (13%)\tLoss: 0.583925\n",
      "Train Epoch: 5 --> 12800/50000 (26%)\tLoss: 0.595949\n",
      "Train Epoch: 5 --> 19200/50000 (38%)\tLoss: 0.579289\n",
      "Train Epoch: 5 --> 25600/50000 (51%)\tLoss: 0.592613\n",
      "Train Epoch: 5 --> 32000/50000 (64%)\tLoss: 0.561306\n",
      "Train Epoch: 5 --> 38400/50000 (77%)\tLoss: 0.691866\n",
      "Train Epoch: 5 --> 44800/50000 (90%)\tLoss: 0.525035\n",
      "\n",
      "Loss: 0.6125 \t Val Accuracy: 7900/10000 (79.000%)\n",
      "\n",
      "Train Epoch: 6 --> 6400/50000 (13%)\tLoss: 0.458198\n",
      "Train Epoch: 6 --> 12800/50000 (26%)\tLoss: 0.488698\n",
      "Train Epoch: 6 --> 19200/50000 (38%)\tLoss: 0.783651\n",
      "Train Epoch: 6 --> 25600/50000 (51%)\tLoss: 0.483868\n",
      "Train Epoch: 6 --> 32000/50000 (64%)\tLoss: 0.726280\n",
      "Train Epoch: 6 --> 38400/50000 (77%)\tLoss: 0.548236\n",
      "Train Epoch: 6 --> 44800/50000 (90%)\tLoss: 0.661318\n",
      "\n",
      "Loss: 0.5943 \t Val Accuracy: 7952/10000 (79.520%)\n",
      "\n",
      "Train Epoch: 7 --> 6400/50000 (13%)\tLoss: 0.699885\n",
      "Train Epoch: 7 --> 12800/50000 (26%)\tLoss: 0.347694\n",
      "Train Epoch: 7 --> 19200/50000 (38%)\tLoss: 0.445519\n",
      "Train Epoch: 7 --> 25600/50000 (51%)\tLoss: 0.423996\n",
      "Train Epoch: 7 --> 32000/50000 (64%)\tLoss: 0.423173\n",
      "Train Epoch: 7 --> 38400/50000 (77%)\tLoss: 0.506280\n",
      "Train Epoch: 7 --> 44800/50000 (90%)\tLoss: 0.447660\n",
      "\n",
      "Loss: 0.5697 \t Val Accuracy: 8040/10000 (80.400%)\n",
      "\n",
      "Train Epoch: 8 --> 6400/50000 (13%)\tLoss: 0.561977\n",
      "Train Epoch: 8 --> 12800/50000 (26%)\tLoss: 0.513065\n",
      "Train Epoch: 8 --> 19200/50000 (38%)\tLoss: 0.516019\n",
      "Train Epoch: 8 --> 25600/50000 (51%)\tLoss: 0.554551\n",
      "Train Epoch: 8 --> 32000/50000 (64%)\tLoss: 0.354134\n",
      "Train Epoch: 8 --> 38400/50000 (77%)\tLoss: 0.556004\n",
      "Train Epoch: 8 --> 44800/50000 (90%)\tLoss: 0.427552\n",
      "\n",
      "Loss: 0.5757 \t Val Accuracy: 8055/10000 (80.550%)\n",
      "\n",
      "Train Epoch: 9 --> 6400/50000 (13%)\tLoss: 0.665020\n",
      "Train Epoch: 9 --> 12800/50000 (26%)\tLoss: 0.365310\n",
      "Train Epoch: 9 --> 19200/50000 (38%)\tLoss: 0.493100\n",
      "Train Epoch: 9 --> 25600/50000 (51%)\tLoss: 0.230392\n",
      "Train Epoch: 9 --> 32000/50000 (64%)\tLoss: 0.630485\n",
      "Train Epoch: 9 --> 38400/50000 (77%)\tLoss: 0.451240\n",
      "Train Epoch: 9 --> 44800/50000 (90%)\tLoss: 0.449525\n",
      "\n",
      "Loss: 0.5783 \t Val Accuracy: 8094/10000 (80.940%)\n",
      "\n",
      "Train Epoch: 10 --> 6400/50000 (13%)\tLoss: 0.408648\n",
      "Train Epoch: 10 --> 12800/50000 (26%)\tLoss: 0.271136\n",
      "Train Epoch: 10 --> 19200/50000 (38%)\tLoss: 0.300934\n",
      "Train Epoch: 10 --> 25600/50000 (51%)\tLoss: 0.231809\n",
      "Train Epoch: 10 --> 32000/50000 (64%)\tLoss: 0.291565\n",
      "Train Epoch: 10 --> 38400/50000 (77%)\tLoss: 0.227098\n",
      "Train Epoch: 10 --> 44800/50000 (90%)\tLoss: 0.504377\n",
      "\n",
      "Loss: 0.5460 \t Val Accuracy: 8192/10000 (81.920%)\n",
      "\n",
      "Train Epoch: 11 --> 6400/50000 (13%)\tLoss: 0.436273\n",
      "Train Epoch: 11 --> 12800/50000 (26%)\tLoss: 0.246706\n",
      "Train Epoch: 11 --> 19200/50000 (38%)\tLoss: 0.140214\n",
      "Train Epoch: 11 --> 25600/50000 (51%)\tLoss: 0.391827\n",
      "Train Epoch: 11 --> 32000/50000 (64%)\tLoss: 0.158699\n",
      "Train Epoch: 11 --> 38400/50000 (77%)\tLoss: 0.252549\n",
      "Train Epoch: 11 --> 44800/50000 (90%)\tLoss: 0.318364\n",
      "\n",
      "Loss: 0.5616 \t Val Accuracy: 8224/10000 (82.240%)\n",
      "\n",
      "Train Epoch: 12 --> 6400/50000 (13%)\tLoss: 0.307124\n",
      "Train Epoch: 12 --> 12800/50000 (26%)\tLoss: 0.296077\n",
      "Train Epoch: 12 --> 19200/50000 (38%)\tLoss: 0.234155\n",
      "Train Epoch: 12 --> 25600/50000 (51%)\tLoss: 0.505879\n",
      "Train Epoch: 12 --> 32000/50000 (64%)\tLoss: 0.294947\n",
      "Train Epoch: 12 --> 38400/50000 (77%)\tLoss: 0.360774\n",
      "Train Epoch: 12 --> 44800/50000 (90%)\tLoss: 0.238748\n",
      "\n",
      "Loss: 0.5461 \t Val Accuracy: 8257/10000 (82.570%)\n",
      "\n",
      "Train Epoch: 13 --> 6400/50000 (13%)\tLoss: 0.225951\n",
      "Train Epoch: 13 --> 12800/50000 (26%)\tLoss: 0.216952\n",
      "Train Epoch: 13 --> 19200/50000 (38%)\tLoss: 0.170347\n",
      "Train Epoch: 13 --> 25600/50000 (51%)\tLoss: 0.211942\n",
      "Train Epoch: 13 --> 32000/50000 (64%)\tLoss: 0.349317\n",
      "Train Epoch: 13 --> 38400/50000 (77%)\tLoss: 0.385079\n",
      "Train Epoch: 13 --> 44800/50000 (90%)\tLoss: 0.284387\n",
      "\n",
      "Loss: 0.5594 \t Val Accuracy: 8240/10000 (82.400%)\n",
      "\n",
      "Train Epoch: 14 --> 6400/50000 (13%)\tLoss: 0.105139\n",
      "Train Epoch: 14 --> 12800/50000 (26%)\tLoss: 0.257636\n",
      "Train Epoch: 14 --> 19200/50000 (38%)\tLoss: 0.282773\n",
      "Train Epoch: 14 --> 25600/50000 (51%)\tLoss: 0.406069\n",
      "Train Epoch: 14 --> 32000/50000 (64%)\tLoss: 0.224910\n",
      "Train Epoch: 14 --> 38400/50000 (77%)\tLoss: 0.136606\n",
      "Train Epoch: 14 --> 44800/50000 (90%)\tLoss: 0.273699\n",
      "\n",
      "Loss: 0.5629 \t Val Accuracy: 8248/10000 (82.480%)\n",
      "\n",
      "Train Epoch: 15 --> 6400/50000 (13%)\tLoss: 0.165812\n",
      "Train Epoch: 15 --> 12800/50000 (26%)\tLoss: 0.202462\n",
      "Train Epoch: 15 --> 19200/50000 (38%)\tLoss: 0.346623\n",
      "Train Epoch: 15 --> 25600/50000 (51%)\tLoss: 0.220703\n",
      "Train Epoch: 15 --> 32000/50000 (64%)\tLoss: 0.372125\n",
      "Train Epoch: 15 --> 38400/50000 (77%)\tLoss: 0.202217\n",
      "Train Epoch: 15 --> 44800/50000 (90%)\tLoss: 0.224063\n",
      "\n",
      "Loss: 0.5781 \t Val Accuracy: 8273/10000 (82.730%)\n",
      "\n",
      "Train Epoch: 16 --> 6400/50000 (13%)\tLoss: 0.358015\n",
      "Train Epoch: 16 --> 12800/50000 (26%)\tLoss: 0.240185\n",
      "Train Epoch: 16 --> 19200/50000 (38%)\tLoss: 0.126179\n",
      "Train Epoch: 16 --> 25600/50000 (51%)\tLoss: 0.269047\n",
      "Train Epoch: 16 --> 32000/50000 (64%)\tLoss: 0.153175\n",
      "Train Epoch: 16 --> 38400/50000 (77%)\tLoss: 0.156122\n",
      "Train Epoch: 16 --> 44800/50000 (90%)\tLoss: 0.185636\n",
      "\n",
      "Loss: 0.5855 \t Val Accuracy: 8261/10000 (82.610%)\n",
      "\n",
      "Train Epoch: 17 --> 6400/50000 (13%)\tLoss: 0.277276\n",
      "Train Epoch: 17 --> 12800/50000 (26%)\tLoss: 0.228292\n",
      "Train Epoch: 17 --> 19200/50000 (38%)\tLoss: 0.292777\n",
      "Train Epoch: 17 --> 25600/50000 (51%)\tLoss: 0.215258\n",
      "Train Epoch: 17 --> 32000/50000 (64%)\tLoss: 0.221138\n",
      "Train Epoch: 17 --> 38400/50000 (77%)\tLoss: 0.347071\n",
      "Train Epoch: 17 --> 44800/50000 (90%)\tLoss: 0.363348\n",
      "\n",
      "Loss: 0.5876 \t Val Accuracy: 8273/10000 (82.730%)\n",
      "\n",
      "Train Epoch: 18 --> 6400/50000 (13%)\tLoss: 0.422730\n",
      "Train Epoch: 18 --> 12800/50000 (26%)\tLoss: 0.163925\n",
      "Train Epoch: 18 --> 19200/50000 (38%)\tLoss: 0.435659\n",
      "Train Epoch: 18 --> 25600/50000 (51%)\tLoss: 0.175061\n",
      "Train Epoch: 18 --> 32000/50000 (64%)\tLoss: 0.225333\n",
      "Train Epoch: 18 --> 38400/50000 (77%)\tLoss: 0.212767\n",
      "Train Epoch: 18 --> 44800/50000 (90%)\tLoss: 0.300695\n",
      "\n",
      "Loss: 0.6357 \t Val Accuracy: 8200/10000 (82.000%)\n",
      "\n",
      "Train Epoch: 19 --> 6400/50000 (13%)\tLoss: 0.252269\n",
      "Train Epoch: 19 --> 12800/50000 (26%)\tLoss: 0.104564\n",
      "Train Epoch: 19 --> 19200/50000 (38%)\tLoss: 0.173562\n",
      "Train Epoch: 19 --> 25600/50000 (51%)\tLoss: 0.350088\n",
      "Train Epoch: 19 --> 32000/50000 (64%)\tLoss: 0.199929\n",
      "Train Epoch: 19 --> 38400/50000 (77%)\tLoss: 0.252911\n",
      "Train Epoch: 19 --> 44800/50000 (90%)\tLoss: 0.178888\n",
      "\n",
      "Loss: 0.6118 \t Val Accuracy: 8263/10000 (82.630%)\n",
      "\n",
      "Train Epoch: 20 --> 6400/50000 (13%)\tLoss: 0.136839\n",
      "Train Epoch: 20 --> 12800/50000 (26%)\tLoss: 0.126198\n",
      "Train Epoch: 20 --> 19200/50000 (38%)\tLoss: 0.148856\n",
      "Train Epoch: 20 --> 25600/50000 (51%)\tLoss: 0.139099\n",
      "Train Epoch: 20 --> 32000/50000 (64%)\tLoss: 0.118629\n",
      "Train Epoch: 20 --> 38400/50000 (77%)\tLoss: 0.269282\n",
      "Train Epoch: 20 --> 44800/50000 (90%)\tLoss: 0.061962\n",
      "\n",
      "Loss: 0.6378 \t Val Accuracy: 8258/10000 (82.580%)\n",
      "\n",
      "Train Epoch: 21 --> 6400/50000 (13%)\tLoss: 0.111379\n",
      "Train Epoch: 21 --> 12800/50000 (26%)\tLoss: 0.051273\n",
      "Train Epoch: 21 --> 19200/50000 (38%)\tLoss: 0.046353\n",
      "Train Epoch: 21 --> 25600/50000 (51%)\tLoss: 0.126745\n",
      "Train Epoch: 21 --> 32000/50000 (64%)\tLoss: 0.234853\n",
      "Train Epoch: 21 --> 38400/50000 (77%)\tLoss: 0.206768\n",
      "Train Epoch: 21 --> 44800/50000 (90%)\tLoss: 0.183810\n",
      "\n",
      "Loss: 0.6466 \t Val Accuracy: 8295/10000 (82.950%)\n",
      "\n",
      "Train Epoch: 22 --> 6400/50000 (13%)\tLoss: 0.073824\n",
      "Train Epoch: 22 --> 12800/50000 (26%)\tLoss: 0.089197\n",
      "Train Epoch: 22 --> 19200/50000 (38%)\tLoss: 0.183452\n",
      "Train Epoch: 22 --> 25600/50000 (51%)\tLoss: 0.117055\n",
      "Train Epoch: 22 --> 32000/50000 (64%)\tLoss: 0.143819\n",
      "Train Epoch: 22 --> 38400/50000 (77%)\tLoss: 0.104460\n",
      "Train Epoch: 22 --> 44800/50000 (90%)\tLoss: 0.055761\n",
      "\n",
      "Loss: 0.6507 \t Val Accuracy: 8329/10000 (83.290%)\n",
      "\n",
      "Train Epoch: 23 --> 6400/50000 (13%)\tLoss: 0.082853\n",
      "Train Epoch: 23 --> 12800/50000 (26%)\tLoss: 0.146542\n",
      "Train Epoch: 23 --> 19200/50000 (38%)\tLoss: 0.043787\n",
      "Train Epoch: 23 --> 25600/50000 (51%)\tLoss: 0.118525\n",
      "Train Epoch: 23 --> 32000/50000 (64%)\tLoss: 0.143470\n",
      "Train Epoch: 23 --> 38400/50000 (77%)\tLoss: 0.057251\n",
      "Train Epoch: 23 --> 44800/50000 (90%)\tLoss: 0.114004\n",
      "\n",
      "Loss: 0.6787 \t Val Accuracy: 8246/10000 (82.460%)\n",
      "\n",
      "Train Epoch: 24 --> 6400/50000 (13%)\tLoss: 0.074439\n",
      "Train Epoch: 24 --> 12800/50000 (26%)\tLoss: 0.136210\n",
      "Train Epoch: 24 --> 19200/50000 (38%)\tLoss: 0.101526\n",
      "Train Epoch: 24 --> 25600/50000 (51%)\tLoss: 0.094261\n",
      "Train Epoch: 24 --> 32000/50000 (64%)\tLoss: 0.126694\n",
      "Train Epoch: 24 --> 38400/50000 (77%)\tLoss: 0.155873\n",
      "Train Epoch: 24 --> 44800/50000 (90%)\tLoss: 0.025035\n",
      "\n",
      "Loss: 0.6878 \t Val Accuracy: 8256/10000 (82.560%)\n",
      "\n",
      "Train Epoch: 25 --> 6400/50000 (13%)\tLoss: 0.083670\n",
      "Train Epoch: 25 --> 12800/50000 (26%)\tLoss: 0.096758\n",
      "Train Epoch: 25 --> 19200/50000 (38%)\tLoss: 0.101195\n",
      "Train Epoch: 25 --> 25600/50000 (51%)\tLoss: 0.139080\n",
      "Train Epoch: 25 --> 32000/50000 (64%)\tLoss: 0.126264\n",
      "Train Epoch: 25 --> 38400/50000 (77%)\tLoss: 0.134841\n",
      "Train Epoch: 25 --> 44800/50000 (90%)\tLoss: 0.128429\n",
      "\n",
      "Loss: 0.7339 \t Val Accuracy: 8258/10000 (82.580%)\n",
      "\n",
      "Train Epoch: 26 --> 6400/50000 (13%)\tLoss: 0.092225\n",
      "Train Epoch: 26 --> 12800/50000 (26%)\tLoss: 0.081414\n",
      "Train Epoch: 26 --> 19200/50000 (38%)\tLoss: 0.012554\n",
      "Train Epoch: 26 --> 25600/50000 (51%)\tLoss: 0.096041\n",
      "Train Epoch: 26 --> 32000/50000 (64%)\tLoss: 0.156437\n",
      "Train Epoch: 26 --> 38400/50000 (77%)\tLoss: 0.071066\n",
      "Train Epoch: 26 --> 44800/50000 (90%)\tLoss: 0.167357\n",
      "\n",
      "Loss: 0.7234 \t Val Accuracy: 8238/10000 (82.380%)\n",
      "\n",
      "Train Epoch: 27 --> 6400/50000 (13%)\tLoss: 0.134171\n",
      "Train Epoch: 27 --> 12800/50000 (26%)\tLoss: 0.106706\n",
      "Train Epoch: 27 --> 19200/50000 (38%)\tLoss: 0.234812\n",
      "Train Epoch: 27 --> 25600/50000 (51%)\tLoss: 0.112465\n",
      "Train Epoch: 27 --> 32000/50000 (64%)\tLoss: 0.177094\n",
      "Train Epoch: 27 --> 38400/50000 (77%)\tLoss: 0.074889\n",
      "Train Epoch: 27 --> 44800/50000 (90%)\tLoss: 0.117169\n",
      "\n",
      "Loss: 0.7318 \t Val Accuracy: 8264/10000 (82.640%)\n",
      "\n",
      "Train Epoch: 28 --> 6400/50000 (13%)\tLoss: 0.049929\n",
      "Train Epoch: 28 --> 12800/50000 (26%)\tLoss: 0.071139\n",
      "Train Epoch: 28 --> 19200/50000 (38%)\tLoss: 0.172145\n",
      "Train Epoch: 28 --> 25600/50000 (51%)\tLoss: 0.081958\n",
      "Train Epoch: 28 --> 32000/50000 (64%)\tLoss: 0.131438\n",
      "Train Epoch: 28 --> 38400/50000 (77%)\tLoss: 0.108073\n",
      "Train Epoch: 28 --> 44800/50000 (90%)\tLoss: 0.163278\n",
      "\n",
      "Loss: 0.7293 \t Val Accuracy: 8279/10000 (82.790%)\n",
      "\n",
      "Train Epoch: 29 --> 6400/50000 (13%)\tLoss: 0.051781\n",
      "Train Epoch: 29 --> 12800/50000 (26%)\tLoss: 0.081796\n",
      "Train Epoch: 29 --> 19200/50000 (38%)\tLoss: 0.074093\n",
      "Train Epoch: 29 --> 25600/50000 (51%)\tLoss: 0.081593\n",
      "Train Epoch: 29 --> 32000/50000 (64%)\tLoss: 0.087660\n",
      "Train Epoch: 29 --> 38400/50000 (77%)\tLoss: 0.100021\n",
      "Train Epoch: 29 --> 44800/50000 (90%)\tLoss: 0.096060\n",
      "\n",
      "Loss: 0.7573 \t Val Accuracy: 8257/10000 (82.570%)\n",
      "\n",
      "Train Epoch: 30 --> 6400/50000 (13%)\tLoss: 0.085267\n",
      "Train Epoch: 30 --> 12800/50000 (26%)\tLoss: 0.035818\n",
      "Train Epoch: 30 --> 19200/50000 (38%)\tLoss: 0.141331\n",
      "Train Epoch: 30 --> 25600/50000 (51%)\tLoss: 0.018840\n",
      "Train Epoch: 30 --> 32000/50000 (64%)\tLoss: 0.052397\n",
      "Train Epoch: 30 --> 38400/50000 (77%)\tLoss: 0.200808\n",
      "Train Epoch: 30 --> 44800/50000 (90%)\tLoss: 0.016902\n",
      "\n",
      "Loss: 0.7429 \t Val Accuracy: 8295/10000 (82.950%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bill\\anaconda3\\envs\\DeepLearning\\lib\\site-packages\\torch\\nn\\_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 30\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    train(i)\n",
    "    evaluate(test_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## `Model accuracy --> 83%!`\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "deeplearning",
   "language": "python",
   "display_name": "DeepLearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}