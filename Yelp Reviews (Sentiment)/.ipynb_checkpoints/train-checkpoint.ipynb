{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import yaml\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython import display\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open('hyperparameters.yaml') as f:\n",
    "    hp = yaml.safe_load(f)['hyperparameters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, df, vectorizer):\n",
    "        print('ReviewDS class instantiated.')\n",
    "\n",
    "        self.df = df\n",
    "        self.vectorizer = vectorizer\n",
    "\n",
    "        self.train_df = self.df[self.df[\"split\"] == 'train']\n",
    "        self.val_df = self.df[self.df[\"split\"] == 'val']\n",
    "        self.test_df = self.df[self.df[\"split\"] == 'test']\n",
    "\n",
    "        self.train_len = len(self.train_df)\n",
    "        self.val_len = len(self.val_df)\n",
    "        self.test_len = len(self.test_df)\n",
    "        print(f'Train len: {self.train_len}\\nVal len: {self.val_len}\\nTest len: {self.test_len}')\n",
    "\n",
    "        self.lookup_dict = {\n",
    "            'train': (self.train_df, self.train_len),\n",
    "            'val': (self.val_df, self.val_len),\n",
    "            'test': (self.test_df, self.test_len),\n",
    "        }\n",
    "\n",
    "        self.set_split('train')\n",
    "\n",
    "    def set_split(self, split):\n",
    "        self.target_split = split\n",
    "        self.target_df, self.target_len = self.lookup_dict[self.target_split]\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def make_vectorizer(cls, csv):\n",
    "        review_df = pd.read_csv(csv)\n",
    "        train_df = review_df[review_df.split=='train']\n",
    "        return cls(\n",
    "            review_df\n",
    "            , ReviewVectorizer.from_df(train_df)\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_vectorizer(cls, csv, vectorizer_path):\n",
    "        review_df = pd.read_csv(csv)\n",
    "        with open vectorizer as f:\n",
    "            vectorizer = ReviewVectorizer.from_serializable(json.load(f))\n",
    "        return cls(\n",
    "            review_df\n",
    "            , vectorizer\n",
    "        )\n",
    "    \n",
    "    def save_vectorizer(self, vectorizer_path):\n",
    "        with open(vectorizer_path, 'w') as f:\n",
    "            json.dump(self.vectorizer.to_serializable(), f)\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        return self.vectorizer\n",
    "    \n",
    "    def get_batch(self, batch_size):\n",
    "        return len(self) // batch_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.target_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.target_df.iloc[idx]\n",
    "        review_vector = self.vectorizer.vectorize(row['review'])\n",
    "        rating_idx = self.vectorizer.rating_vocab.lookup_token(row['rating'])\n",
    "        return {\n",
    "            'X': review_vector,\n",
    "            'y': rating_idx\n",
    "        }\n",
    "    \n",
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    A generator function which wraps the PyTorch DataLoader. It will \n",
    "      ensure each tensor is on the write device location.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ReviewVectorizer():\n",
    "    def __init__(self, review_vocab, rating_vocab):\n",
    "        self.review_vocab = review_vocab\n",
    "        self.rating_vocab = rating_vocab\n",
    "\n",
    "    def vectorize(self, text):\n",
    "        encoding = np.zeros(len(self.review_vocab), dtype=np.float32)\n",
    "        for word in text.split(' '):\n",
    "            if word not in string.punctuation:\n",
    "                encoding[self.review_vocab.lookup_token(word)] = 1\n",
    "        return encoding\n",
    "\n",
    "    def from_df(self, df, length):\n",
    "        review_vocab = Vocabulary(UNK=True)\n",
    "        rating_vocab = Vocabulary(UNK=False)\n",
    "        for rating in sorted(set(df['rating'])):\n",
    "            rating_vocab.add_token(rating)\n",
    "        \n",
    "        word_counts\n",
    "\n",
    "    def from_serializable(self):\n",
    "        pass\n",
    "\n",
    "    def to_serializable(self):\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def to_serializable(self):\n",
    "        pass\n",
    "    \n",
    "    def from_serializable(cls):\n",
    "        pass\n",
    "    \n",
    "    def add_token(self):\n",
    "        pass\n",
    "    \n",
    "    def add_tokens(self):\n",
    "        pass\n",
    "    \n",
    "    def lookup_token(self):\n",
    "        pass\n",
    "    \n",
    "    def lookup_index(self):\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        return(len(token_to_idx))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
